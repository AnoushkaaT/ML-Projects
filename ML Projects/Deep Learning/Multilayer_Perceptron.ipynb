{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qZJE0GriIutA"},"outputs":[],"source":["from sklearn.datasets import load_wine\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import numpy as np\n","import pandas as pd\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","metadata":{"id":"Qjp7tT6dJ8pp"},"source":["- from sklearn.datasets import load_wine: This imports the load_wine function -from scikit-learn's datasets module, which allows us to load the Wine dataset.\n","-from sklearn.model_selection import train_test_split: This imports the train_test_split function from scikit-learn's model_selection module, which helps us split the dataset into training and testing sets.\n","-from sklearn.preprocessing import StandardScaler: This imports the StandardScaler class from scikit-learn's preprocessing module, which is used to standardize the dataset by removing the mean and scaling to unit variance.\n","-import numpy as np: This imports the NumPy library, which provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9l8EgOPIupJ"},"outputs":[],"source":["# Load the wine dataset\n","data = load_wine()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1708708350110,"user":{"displayName":"Adiba Muskan","userId":"14637898379681137009"},"user_tz":-330},"id":"BekxlnYkLSWu","outputId":"3b61bab5-ecd9-4b68-a605-b357af67c898"},"outputs":[{"output_type":"stream","name":"stdout","text":["   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n","0    14.23        1.71  2.43               15.6      127.0           2.80   \n","1    13.20        1.78  2.14               11.2      100.0           2.65   \n","2    13.16        2.36  2.67               18.6      101.0           2.80   \n","3    14.37        1.95  2.50               16.8      113.0           3.85   \n","4    13.24        2.59  2.87               21.0      118.0           2.80   \n","\n","   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n","0        3.06                  0.28             2.29             5.64  1.04   \n","1        2.76                  0.26             1.28             4.38  1.05   \n","2        3.24                  0.30             2.81             5.68  1.03   \n","3        3.49                  0.24             2.18             7.80  0.86   \n","4        2.69                  0.39             1.82             4.32  1.04   \n","\n","   od280/od315_of_diluted_wines  proline  \n","0                          3.92   1065.0  \n","1                          3.40   1050.0  \n","2                          3.17   1185.0  \n","3                          3.45   1480.0  \n","4                          2.93    735.0  \n"]}],"source":["# data = pd.DataFrame(data.data, columns=data.feature_names)\n","# print(data.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":427,"status":"error","timestamp":1708708466339,"user":{"displayName":"Adiba Muskan","userId":"14637898379681137009"},"user_tz":-330},"id":"Sz7ZTw0aIulX","outputId":"3bbc9fcc-cd44-4ee4-bafd-98a7bea75557"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'DataFrame' object has no attribute 'data'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-72150f5a6741>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Split the dataset into training, validation, and test sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"]}],"source":["x,y = data.data, data.target\n","# Split the dataset into training, validation, and test sets\n","x_train,x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, random_state=42) #training set\n","x_val,x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42) #validation set"]},{"cell_type":"markdown","metadata":{"id":"rJ3kheCoLPAY"},"source":["-This splits the dataset into training and testing sets. Here, 30% of the data is reserved for testing, and random_state=42 sets a seed for reproducibility."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZEC6YtPvOSPe"},"outputs":[],"source":["print(data.target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMr46kABIuiS"},"outputs":[],"source":["# Standardize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"ZTm1p5bDLeBH"},"source":["- scaler = StandardScaler(): This initializes a StandardScaler object.\n","- X_train = scaler.fit_transform(X_train): This standardizes the training features using the fit_transform method, which computes the mean and standard deviation of each feature and then scales the features.\n","- X_val = scaler.transform(X_val): This standardizes the validation features using the previously computed mean and standard deviation from the training set.\n","- X_test = scaler.transform(X_test): This standardizes the test features using the same mean and standard deviation computed from the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WPG81lqnIue3"},"outputs":[],"source":["# Defining the model architecture\n","model = keras.Sequential([\n","    layers.Dense(64, activation=\"relu\", input_dim=X.shape[1]),  # Hidden layer with 64 neurons and ReLU activation\n","    layers.Dense(32, activation=\"relu\"),                        # Another hidden layer with 32 neurons and ReLU activation\n","    layers.Dense(3, activation=\"softmax\")                       # Output layer with softmax activation for multiclass classification\n","])"]},{"cell_type":"markdown","metadata":{"id":"XVtO_dp4MBeI"},"source":["- This defines a sequential model using Keras. We define three layers: two hidden layers with 64 and 32 neurons respectively, and an output layer with 3 neurons (one for each class in the dataset) using softmax activation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Labs0NFOIuby"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) #Adam is an optimization algorithm that adapts the learning rate during training"]},{"cell_type":"markdown","metadata":{"id":"7fY5taW4MVqr"},"source":["- This compiles the model. We specify the Adam optimizer, sparse categorical cross-entropy loss function (suitable for multiclass classification tasks), and accuracy as the evaluation metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOBn_NheIuYu"},"outputs":[],"source":["# Train the model\n","model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"]},{"cell_type":"markdown","metadata":{"id":"fnmJxwo0MlIf"},"source":["- This trains the model on the training data for 10 epochs, using the validation data for validation during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"irh7yit_IuU6"},"outputs":[],"source":["# Evaluate the model on the test set\n","test_loss, test_acc = model.evaluate(X_test, y_test)\n","print(f\"Test accuracy: {test_acc:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"wfUUiBqKM9Kb"},"source":["-  This evaluates the trained model on the test data and computes the test loss and accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0OpL0BXLIuRI"},"outputs":[],"source":["# Make predictions on new data\n","X_new = np.array([\n","    [13.71, 1.86, 2.36, 16.6, 101.2, 2.61, 2.88, 0.27, 1.69, 3.8, 1.11, 4.0, 1035],\n","    [13.56, 1.73, 2.46, 20.5, 116.0, 2.96, 2.78, 0.2, 2.45, 6.25, 0.98, 3.03, 1120]\n","])\n","predictions = model.predict(X_new)\n","print(\"Predictions:\")\n","for i, pred in enumerate(predictions):\n","    print(f\"Sample {i+1}: {pred}\")"]},{"cell_type":"markdown","metadata":{"id":"YNHldOiHNLFO"},"source":["- predictions = model.predict(X_new): This makes predictions on new data (X_new) using the trained model.\n","The print(\"Predictions:\") line prints a header for the predictions.\n","for i, pred in enumerate(predictions): print(f\"Sample {i+1}: {pred}\"): This iterates over the predictions and prints the predicted probabilities for each sample in X_new."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}